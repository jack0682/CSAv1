<!--
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
 â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•

Cognitive Synergy Architecture (CSA)
A next-generation, research-grade cognition stack
for collaborative, human-centric robots
-->

<p align="center">
  <img src="docs/media/csa_banner.svg" width="80%" alt="CSA banner">
</p>

# ğŸ“š Cognitive Synergy Architecture (CSA)

> **Modular cognition for robots that *perceive*, *reason*, and *co-think* with humans.**

CSA is a multistage robotics framework that unifies **semantic perception**, **adaptive planning**, and
**explainable control** under a single, ROS 2â€“native codebase.
It is organized into three tightly-coupled sub-architectures **SEGO**, **IMAGO**, and **LOGOS** that
together deliver a full perception-to-action loop with built-in reasoning, safety, and self-reflection.

| Sub-Architecture | Core Question                 | Key Capabilities (current state)                        |
|------------------|-------------------------------|---------------------------------------------------------|
| **SEGO**<br>*(Semantic Graph Ontology mapper)* | â€œ**What** exactly am I seeing?â€ | â€¢ Real-time RGB-D detection (YOLO v5)<br>â€¢ Multi-object tracking (StrongSORT)<br>â€¢ 6-DoF pose fusion (ORB-SLAM2 / RTAB-Map)<br>â€¢ 3-D semantic mapping + scene-graph export |
| **IMAGO**<br>*(Intent Modeling & Action Generation Operator)* | â€œ**Why** should I act and **how** do I adapt my plan?â€ | â€¢ Natural-language intent parsing (LLM plugin)<br>â€¢ Symbolic & neuro-symbolic planners (HTN, BT, CoT)<br>â€¢ PPO-based self-adaptation + meta-controller |
| **LOGOS**<br>*(Logical Ontological Generator for Self-adjustment)* | â€œDoes my behaviour remain **safe**, **valid**, and **explainable**?â€ | â€¢ Run-time ontology checks (OWL 2, DL-query)<br>â€¢ Policy distillation â†’ decision trees<br>â€¢ Human-readable rationales (XAI report) |

> **Status â€“ 2025-06 14**   SEGO Stage-1 is feature-complete; IMAGO & LOGOS scaffolding is included but
> most PRs are still **WIP**.  
> **Road-mapped milestones** are listed [here Â»](#-roadmap--milestones).

---

## ğŸ“˜ Learn More

This repository is based on the system architecture and cognitive vision proposed in  
my own review papers:

- [_**"Towards Cognitive Collaborative Robots: Semantic-Level Integration and Explainable Control for Human-Centric Cooperation"**_](https://arxiv.org/abs/2505.03815)
- [_**"Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots"**_](https://arxiv.org/abs/2506.13149)

These papers present the conceptual and theoretical foundation for **CSA (Cognitive Synergy Architecture)** â€”  
a modular framework that integrates semantic perception (**SEGO**), intention-aware planning (**IMAGO**),  
and ontological reasoning with explainable control (**LOGOS**).

âœ¨ This project aims to realize the vision outlined in these papers:  
to build robots that not only act, but also understand â€”  
**semantically, ethically, and reflectively**, in collaboration with humans.

---

## ğŸ“‘ Table of Contents
1. [Quick Start](#-quick-start)
1. [High-Level Architecture](#-high-level-architecture)
1. [Detailed Modules](#-module-breakdown-sego-stage-1)
1. [Installation Guide](#-installation--build)
1. [Runtime Walk-through](#-runtime-flow)
1. [Directory Layout](#-repository-layout)
1. [Roadmap & Milestones](#-roadmap--milestones)

---

## ğŸš€ Quick Start

```bash
# 0 Â· Host prerequisites (Ubuntu 22.04 + ROS 2 Humble assumed)
sudo apt update && sudo apt install build-essential git lsb-release curl

# 1 Â· Clone
git clone https://github.com/jack0682/CSA.git
cd CSA && git submodule update --init --recursive

# 2 Â· Setup ROS 2 & Python env
source /opt/ros/humble/setup.bash        # adjust if using another distro
python3 -m venv .venv && source .venv/bin/activate
pip install -U pip wheel
rosdep install --from-paths src -yi      # system deps

# 3 Â· Build (SEGO only)
colcon build --packages-select \
    csa_interfaces csa_yolo_inference csa_slam_interface csa_semantic_mapper
source install/setup.bash

# 4 Â· Run semantic mapping pipeline
ros2 launch csa_launch sego_pipeline.launch.py \
    camera_model:=realsense bag:=false visualize:=true
```

## SEGO System Pipeline
![SEGO System Pipeline](SEGO_System_pipline.png)

## IMAGO System Pipeline
![IMAGO System Pipeline](IMAGO_System_pipline.png)


---

## ğŸ§­ High-Level Architecture

```mermaid
graph TD

%% === SEGO: Semantic Mapping ===
subgraph SEGO [Stage 1: Semantic Mapping]
  YT[yolo_tracker_node] --> OBJS[/tracked_objects/]
  SLAM[slam_pose_node] --> CAM[/camera/pose/]
  OBJS --> MAP[semantic_mapper_node]
  CAM --> MAP
  MAP --> SG[Scene Graph - JSON]
  SG --> MEM[Semantic Memory]
end

%% === IMAGO: Intent Parsing + Planning ===
subgraph IMAGO [Stage 2: Intent Planning]
  INTENT[Intent Parser] --> GGRAPH[Goal Graph]
  MEM --> GGRAPH
  GGRAPH --> PLAN[Trajectory Planner]
end

%% === LOGOS: Reasoning + Control ===
subgraph LOGOS [Stage 3: Ontological Control]
  PLAN --> ACT[Ï• Actuator Commands]
  PLAN --> XAI[Î¨ XAI Rationale]
  XAI --> HRI[HRI UI / Speech]
  ONT[Î© Ontology Rules] -.-> PLAN
end

classDef dim fill:#f9f9f9,stroke:#bbb,stroke-dasharray:5 5,color:#777;
class ONT dim;

```


Data Loop: sensor â†’ semantic map â†’ reasoning â†’ plan â†’ control â†’ explanation

Safety Loop: LOGOS continuously validates each plan slice (Î”t) via OWL-based
rules; if violation, a fallback BT and human prompt are triggered.

## ğŸ”¬ Module Breakdown (SEGO Stage-1)

### 1 Â· csa_yolo_inference

| Feature        | Implementation                           |
| -------------- | ---------------------------------------- |
| Detector       | YOLO v5 6.2 + Ultralytics API            |
| Tracker        | StrongSORT (boxmot fork)                 |
| Depth Sampling | RealSense D435 / any aligned depth topic |
| Output Message | `csa_interfaces/TrackedObjectArray`      |

### 2 Â· csa_slam_interface

Wraps ORB-SLAM2 (stereo/RGB-D) or RTAB-Map.
Publishes geometry_msgs/PoseStamped @ â‰ˆ30 Hz on /camera/pose.
Automatic time-sync with YOLO frames via TF + approx-time-policy.

### 3 Â· csa_semantic_mapper

Projects 2-D bounding boxes into 3-D world frame:
Builds an online scene graph (network-x) and dumps incremental
.json logs per track (ready for Neo4J ingestion).

``` bash
{
  "class"   : "cbn_wooden_box",
  "track_id": 27,
  "position": [ 0.773, -0.142, 1.035 ],
  "size"    : [ 0.28, 0.35, 0.12 ],     // meters
  "pose_cov": [ 5.2e-4, 8.9e-4, â€¦ ],
  "timestamp": 1715530123.447,
  "world_frame": "map",
  "source": {
    "camera_frame": "realsense_link",
    "depth_px": 713
  }
}
```
### 4 Â· Visualization

Launch RViz2 with pre-loaded config: /rviz/sego_live.rviz
Display: TF tree, /tracked_objects markers (ID-color keyed), occupancy map.


## ğŸ–¥ï¸ Installation & Build

| Layer   | Package               | Version         | Install hint                                                                                        |
| ------- | --------------------- | --------------- | --------------------------------------------------------------------------------------------------- |
| System  | `Ubuntu`              | 22.04 LTS       | `apt â€¦`                                                                                             |
| ROS 2   | `Humble Hawksbill`    | â‰¥ 0.11          | [link](https://docs.ros.org/en/humble/)                                                             |
| Vision  | `OpenCV`              | 4.9             | `apt install libopencv-dev`                                                                         |
| ML      | `PyTorch`             | 2.2 + CUDA-11.7 | `pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117` |
| SLAM    | `ORB-SLAM2`           | custom          | included as git submodule                                                                           |
| Tracker | `boxmot`              | 0.2.x           | `pip install boxmot`                                                                                |
| Misc    | `pangolin` / `eigen3` | latest          | `apt install libeigen3-dev libgl1-mesa-dev`                                                         |


 ## ğŸ® Runtime Flow

 | Step | Node                   | Topic                     | Rate  | Note                |
| ---- | ---------------------- | ------------------------- | ----- | ------------------- |
| â‘     | **Sensor**             | `/camera/color/image_raw` | 30 Hz | RealSense RGB       |
| â‘¡    | `yolo_tracker_node`    | `/tracked_objects`        | 15 Hz | after NMS + tracker |
| â‘¢    | `slam_pose_node`       | `/camera/pose`            | 30 Hz | worldâ€“camera TF     |
| â‘£    | `semantic_mapper_node` | `/scene_graph` (latched)  | 2 Hz  | heavy but sparse    |
| â‘¤    | `rviz2`                | markers                   | live  | for debugging       |

## ğŸ“‚ Repository Layout

``` css
CSA/
â”œâ”€â”€ docs/                 â† diagrams, papers, design notes
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ csa_interfaces/   â† ROS 2 msg definitions
â”‚   â”œâ”€â”€ csa_yolo_inference/
â”‚   â”œâ”€â”€ csa_slam_interface/
â”‚   â”œâ”€â”€ csa_semantic_mapper/
â”‚   â”œâ”€â”€ csa_utils/        â† time_sync, coord transforms
â”‚   â”œâ”€â”€ csa_launch/
â”‚   â”œâ”€â”€ imago_core/       â† planner skeleton (Stage 2)
â”‚   â””â”€â”€ logos_core/       â† ontology + XAI (Stage 3)
â”œâ”€â”€ third_party/          â† submodules (ORB-SLAM2, pangolinâ€¦)
â”œâ”€â”€ docker/
â”œâ”€â”€ .devcontainer/        â† VS Code remote config
â””â”€â”€ README.md             â† you are here

```

## ğŸ—ºï¸ Roadmap & Milestones

| Quarter | Theme                           | Deliverable                                    |
| ------- | ------------------------------- | ---------------------------------------------- |
| 2025 Q3 | **IMAGO Î±**                     | HTN planner; LLM intent parser                 |
| 2025 Q4 | **LOGOS Î±**                     | Live policy distillation + OWL safety layer    |
| 2026 Q1 | **Multi-robot** demo            | Cross-agent shared scene graphs                |
| 2026 Q2 | **HRI** pilot                   | Common-ground intent classifier, voice+gesture |
| 2026 Q4 | **Science Robotics** submission | End-to-end human+robots furniture assembly     |

---

# IMAGO ì„¤ê³„ ìˆ˜ì‹ì  Backbone

---

## 1ï¸âƒ£ End-Effector Global Goal

ëª©í‘œ EE pose:
\[
\mathbf{x}_d^{\text{goal}} =
\begin{bmatrix}
x_d \\ y_d \\ z_d \\ q_w \\ q_x \\ q_y \\ q_z
\end{bmatrix}
\]

---

## 2ï¸âƒ£ Arm Move Planning

Arm global joint plan ìƒì„±:
\[
\boldsymbol{\theta}_d^{\text{arm}}(t) =
\mathcal{P}_{\text{arm}}(\mathbf{x}_d^{\text{goal}}, t)
\]

---

## 3ï¸âƒ£ Arm FK â†’ ì˜ˆìƒ EE pose

Arm joint planì„ ê¸°ë°˜ìœ¼ë¡œ EE ì˜ˆìƒ pose ê³„ì‚°:
\[
\mathbf{x}_d^{\text{arm-FK}}(t) = FK(\boldsymbol{\theta}_d^{\text{arm}}(t))
\]

---

## 4ï¸âƒ£ End-Effector Mini Planning (LiDAR ê¸°ë°˜)

EE local plan ìƒì„±:
\[
\mathbf{x}_d^{\text{ee}}(t) =
\mathcal{P}_{\text{ee}}\left( 
\mathbf{x}_d^{\text{arm-FK}}(t),
\mathbf{L}_{\text{ee}}(t),
t
\right)
\]

---

## 5ï¸âƒ£ Plan Synchronizer

Arm FKì™€ EE planì˜ ë™ê¸°í™” ê²€ì¦:
\[
\left\|
\mathbf{x}_d^{\text{ee}}(t) - \mathbf{x}_d^{\text{arm-FK}}(t)
\right\| < \varepsilon_{\text{sync}}
\]

---

## 6ï¸âƒ£ Inverse Kinematics

EE local plan â†’ ìµœì¢… joint target ë³€í™˜:
\[
\boldsymbol{\theta}_d^{\text{final}}(t) = IK(\mathbf{x}_d^{\text{ee}}(t))
\]

---

## 7ï¸âƒ£ DH íŒŒë¼ë¯¸í„° (4DOF â†’ 6DOF í™•ì¥)

### 4DOF ì˜ˆ
\[
T_i =
\begin{bmatrix}
\cos \theta_i & -\sin \theta_i \cos \alpha_i & \sin \theta_i \sin \alpha_i & a_i \cos \theta_i \\
\sin \theta_i & \cos \theta_i \cos \alpha_i & -\cos \theta_i \sin \alpha_i & a_i \sin \theta_i \\
0 & \sin \alpha_i & \cos \alpha_i & d_i \\
0 & 0 & 0 & 1
\end{bmatrix}
\]

### 6DOF í™•ì¥
6ê°œì˜ linkë³„ DH matrix ê³±:
\[
T_{06} = T_1 T_2 T_3 T_4 T_5 T_6
\]

---

## 8ï¸âƒ£ Pole-Zero ML ë³´ìƒê¸°

ML ê¸°ë°˜ pole-zero weight:
\[
C^{\text{ML}}(s) = \operatorname{diag}\left\{
w_{M,i}(t) s^2 + w_{B,i}(t) s
\right\}
\]

ML policy:
\[
\boldsymbol{w}(t) =
\pi_{\text{RL}}\left(
\mathbf{x}_d^{\text{ee}}(t),
\boldsymbol{\theta}(t),
\dot{\boldsymbol{\theta}}(t),
\mathbf{e}(t);
\boldsymbol{\phi}
\right)
\]

---

## 9ï¸âƒ£ Delay Compensation

\[
C_{\text{delay}}(s) = \operatorname{diag}\left\{
\frac{T_i s + 1}{\alpha_i T_i s + 1}
\right\}
\]

---

## ğŸ”Ÿ Final Wrapping Control Law

\[
\boldsymbol{\tau}_{\text{actual}}(s) =
H_{\text{int}}(s)
C^{\text{ML}}(s)
C_{\text{delay}}(s)
e^{+ s \Delta t}
\mathcal{L}\{
\boldsymbol{\theta}_d^{\text{final}}(t)
\}
\]

---

## ğŸŒŒ ì „ì²´ íë¦„

\[
\mathbf{x}_d^{\text{goal}}
\Rightarrow
\boldsymbol{\theta}_d^{\text{arm}}
\Rightarrow
\mathbf{x}_d^{\text{arm-FK}}
\Rightarrow
\mathbf{x}_d^{\text{ee}}
\Rightarrow
\boldsymbol{\theta}_d^{\text{final}}
\Rightarrow
\boldsymbol{\tau}_{\text{actual}}
\]

---

