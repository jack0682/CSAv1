<!--
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë
‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Cognitive Synergy Architecture (CSA)
A next-generation, research-grade cognition stack
for collaborative, human-centric robots
--> 

<p align="center">
  <img src="docs/media/csa_banner.svg" width="80%" alt="CSA banner">
</p>

# üìö Cognitive Synergy Architecture (CSA)

---
## ‚ú® Project Summary

CSA (Cognitive Synergy Architecture) is an advanced ROS 2‚Äìnative framework for human-centric robots.  
It integrates perception (SEGO), planning (IMAGO), and reasoning (LOGOS) for robots that **understand**  
their environment, **adapt** plans, and **explain** decisions in real time.  

This repository contains the codebase, architecture, and demo pipelines for CSA research and development.
---

> **Modular cognition for robots that *perceive*, *reason*, and *co-think* with humans.**

CSA is a multistage robotics framework that unifies **semantic perception**, **adaptive planning**, and
**explainable control** under a single, ROS 2‚Äìnative codebase.
It is organized into three tightly-coupled sub-architectures **SEGO**, **IMAGO**, and **LOGOS** that
together deliver a full perception-to-action loop with built-in reasoning, safety, and self-reflection.

| Sub-Architecture | Core Question                 | Key Capabilities (current state)                        |
|------------------|-------------------------------|---------------------------------------------------------|
| **SEGO**<br>*(Semantic Graph Ontology mapper)* | ‚Äú**What** exactly am I seeing?‚Äù | ‚Ä¢ Real-time RGB-D detection (YOLO v5)<br>‚Ä¢ Multi-object tracking (StrongSORT)<br>‚Ä¢ 6-DoF pose fusion (ORB-SLAM2 / RTAB-Map)<br>‚Ä¢ 3-D semantic mapping + scene-graph export |
| **IMAGO**<br>*(Intent Modeling & Action Generation Operator)* | ‚Äú**Why** should I act and **how** do I adapt my plan?‚Äù | ‚Ä¢ Natural-language intent parsing (LLM plugin)<br>‚Ä¢ Symbolic & neuro-symbolic planners (HTN, BT, CoT)<br>‚Ä¢ PPO-based self-adaptation + meta-controller |
| **LOGOS**<br>*(Logical Ontological Generator for Self-adjustment)* | ‚ÄúDoes my behaviour remain **safe**, **valid**, and **explainable**?‚Äù | ‚Ä¢ Run-time ontology checks (OWL 2, DL-query)<br>‚Ä¢ Policy distillation ‚Üí decision trees<br>‚Ä¢ Human-readable rationales (XAI report) |

> **Status ‚Äì 2025-06 14**   SEGO Stage-1 is feature-complete; IMAGO & LOGOS scaffolding is included but
> most PRs are still **WIP**.  
> **Road-mapped milestones** are listed [here ¬ª](#-roadmap--milestones).

---

## üìò Learn More

This repository is based on the system architecture and cognitive vision proposed in  
my own review papers:

- [_**"Towards Cognitive Collaborative Robots: Semantic-Level Integration and Explainable Control for Human-Centric Cooperation"**_](https://arxiv.org/abs/2505.03815)
- [_**"Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots"**_](https://arxiv.org/abs/2506.13149)

These papers present the conceptual and theoretical foundation for **CSA (Cognitive Synergy Architecture)** ‚Äî  
a modular framework that integrates semantic perception (**SEGO**), intention-aware planning (**IMAGO**),  
and ontological reasoning with explainable control (**LOGOS**).

‚ú® This project aims to realize the vision outlined in these papers:  
to build robots that not only act, but also understand ‚Äî  
**semantically, ethically, and reflectively**, in collaboration with humans.

---

## üìë Table of Contents
1. [Quick Start](#-quick-start)
1. [High-Level Architecture](#-high-level-architecture)
1. [Detailed Modules](#-module-breakdown-sego-stage-1)
1. [Installation Guide](#-installation--build)
1. [Runtime Walk-through](#-runtime-flow)
1. [Directory Layout](#-repository-layout)
1. [Roadmap & Milestones](#-roadmap--milestones)

---

## üöÄ Quick Start

```bash
# 0 ¬∑ Host prerequisites (Ubuntu 22.04 + ROS 2 Humble assumed)
sudo apt update && sudo apt install build-essential git lsb-release curl

# 1 ¬∑ Clone
git clone https://github.com/jack0682/CSA.git
cd CSA && git submodule update --init --recursive

# 2 ¬∑ Setup ROS 2 & Python env
source /opt/ros/humble/setup.bash        # adjust if using another distro
python3 -m venv .venv && source .venv/bin/activate
pip install -U pip wheel
rosdep install --from-paths src -yi      # system deps

# 3 ¬∑ Build (SEGO only)
colcon build --packages-select \
    csa_interfaces csa_yolo_inference csa_slam_interface csa_semantic_mapper
source install/setup.bash

# 4 ¬∑ Run semantic mapping pipeline
ros2 launch csa_launch sego_pipeline.launch.py \
    camera_model:=realsense bag:=false visualize:=true
```

## SEGO System Pipeline
![SEGO System Pipeline](SEGO_System_pipline.png)

## IMAGO System Pipeline
![IMAGO System Pipeline](IMAGO_System_pipline.png)


---

## üß≠ High-Level Architecture

```mermaid
graph TD

%% === SEGO: Semantic Mapping ===
subgraph SEGO [Stage 1: Semantic Mapping]
  YT[yolo_tracker_node] --> OBJS[/tracked_objects/]
  SLAM[slam_pose_node] --> CAM[/camera/pose/]
  OBJS --> MAP[semantic_mapper_node]
  CAM --> MAP
  MAP --> SG[Scene Graph - JSON]
  SG --> MEM[Semantic Memory]
end

%% === IMAGO: Intent Parsing + Planning ===
subgraph IMAGO [Stage 2: Intent Planning]
  INTENT[Intent Parser] --> GGRAPH[Goal Graph]
  MEM --> GGRAPH
  GGRAPH --> PLAN[Trajectory Planner]
end

%% === LOGOS: Reasoning + Control ===
subgraph LOGOS [Stage 3: Ontological Control]
  PLAN --> ACT[œï Actuator Commands]
  PLAN --> XAI[Œ® XAI Rationale]
  XAI --> HRI[HRI UI / Speech]
  ONT[Œ© Ontology Rules] -.-> PLAN
end

classDef dim fill:#f9f9f9,stroke:#bbb,stroke-dasharray:5 5,color:#777;
class ONT dim;

```


Data Loop: sensor ‚Üí semantic map ‚Üí reasoning ‚Üí plan ‚Üí control ‚Üí explanation

Safety Loop: LOGOS continuously validates each plan slice (Œît) via OWL-based
rules; if violation, a fallback BT and human prompt are triggered.

## üî¨ Module Breakdown (SEGO Stage-1)

### 1 ¬∑ csa_yolo_inference

| Feature        | Implementation                           |
| -------------- | ---------------------------------------- |
| Detector       | YOLO v5 6.2 + Ultralytics API            |
| Tracker        | StrongSORT (boxmot fork)                 |
| Depth Sampling | RealSense D435 / any aligned depth topic |
| Output Message | `csa_interfaces/TrackedObjectArray`      |

### 2 ¬∑ csa_slam_interface

Wraps ORB-SLAM2 (stereo/RGB-D) or RTAB-Map.
Publishes geometry_msgs/PoseStamped @ ‚âà30 Hz on /camera/pose.
Automatic time-sync with YOLO frames via TF + approx-time-policy.

### 3 ¬∑ csa_semantic_mapper

Projects 2-D bounding boxes into 3-D world frame:
Builds an online scene graph (network-x) and dumps incremental
.json logs per track (ready for Neo4J ingestion).

``` bash
{
  "class"   : "cbn_wooden_box",
  "track_id": 27,
  "position": [ 0.773, -0.142, 1.035 ],
  "size"    : [ 0.28, 0.35, 0.12 ],     // meters
  "pose_cov": [ 5.2e-4, 8.9e-4, ‚Ä¶ ],
  "timestamp": 1715530123.447,
  "world_frame": "map",
  "source": {
    "camera_frame": "realsense_link",
    "depth_px": 713
  }
}
```
### 4 ¬∑ Visualization

Launch RViz2 with pre-loaded config: /rviz/sego_live.rviz
Display: TF tree, /tracked_objects markers (ID-color keyed), occupancy map.


## üñ•Ô∏è Installation & Build

| Layer   | Package               | Version         | Install hint                                                                                        |
| ------- | --------------------- | --------------- | --------------------------------------------------------------------------------------------------- |
| System  | `Ubuntu`              | 22.04 LTS       | `apt ‚Ä¶`                                                                                             |
| ROS 2   | `Humble Hawksbill`    | ‚â• 0.11          | [link](https://docs.ros.org/en/humble/)                                                             |
| Vision  | `OpenCV`              | 4.9             | `apt install libopencv-dev`                                                                         |
| ML      | `PyTorch`             | 2.2 + CUDA-11.7 | `pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117` |
| SLAM    | `ORB-SLAM2`           | custom          | included as git submodule                                                                           |
| Tracker | `boxmot`              | 0.2.x           | `pip install boxmot`                                                                                |
| Misc    | `pangolin` / `eigen3` | latest          | `apt install libeigen3-dev libgl1-mesa-dev`                                                         |


 ## üéÆ Runtime Flow

 | Step | Node                   | Topic                     | Rate  | Note                |
| ---- | ---------------------- | ------------------------- | ----- | ------------------- |
| ‚ë†    | **Sensor**             | `/camera/color/image_raw` | 30 Hz | RealSense RGB       |
| ‚ë°    | `yolo_tracker_node`    | `/tracked_objects`        | 15 Hz | after NMS + tracker |
| ‚ë¢    | `slam_pose_node`       | `/camera/pose`            | 30 Hz | world‚Äìcamera TF     |
| ‚ë£    | `semantic_mapper_node` | `/scene_graph` (latched)  | 2 Hz  | heavy but sparse    |
| ‚ë§    | `rviz2`                | markers                   | live  | for debugging       |

## üìÇ Repository Layout

``` css
CSA/
‚îú‚îÄ‚îÄ docs/                 ‚Üê diagrams, papers, design notes
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ csa_interfaces/   ‚Üê ROS 2 msg definitions
‚îÇ   ‚îú‚îÄ‚îÄ csa_yolo_inference/
‚îÇ   ‚îú‚îÄ‚îÄ csa_slam_interface/
‚îÇ   ‚îú‚îÄ‚îÄ csa_semantic_mapper/
‚îÇ   ‚îú‚îÄ‚îÄ csa_utils/        ‚Üê time_sync, coord transforms
‚îÇ   ‚îú‚îÄ‚îÄ csa_launch/
‚îÇ   ‚îú‚îÄ‚îÄ imago_core/       ‚Üê planner skeleton (Stage 2)
‚îÇ   ‚îî‚îÄ‚îÄ logos_core/       ‚Üê ontology + XAI (Stage 3)
‚îú‚îÄ‚îÄ third_party/          ‚Üê submodules (ORB-SLAM2, pangolin‚Ä¶)
‚îú‚îÄ‚îÄ docker/
‚îú‚îÄ‚îÄ .devcontainer/        ‚Üê VS Code remote config
‚îî‚îÄ‚îÄ README.md             ‚Üê you are here

```

## üó∫Ô∏è Roadmap & Milestones

| Quarter | Theme                           | Deliverable                                    |
| ------- | ------------------------------- | ---------------------------------------------- |
| 2025 Q3 | **IMAGO Œ±**                     | HTN planner; LLM intent parser                 |
| 2025 Q4 | **LOGOS Œ±**                     | Live policy distillation + OWL safety layer    |
| 2026 Q1 | **Multi-robot** demo            | Cross-agent shared scene graphs                |
| 2026 Q2 | **HRI** pilot                   | Common-ground intent classifier, voice+gesture |
| 2026 Q4 | **Science Robotics** submission | End-to-end human+robots furniture assembly     |

---

# IMAGO Control Architecture Backbone

---

## ‚öôÔ∏è IMAGO + Panda Integrated Design

![IMAGO+Panda design Draft](IMAGOPanda_design_Draft.png) 

---

## ‚öôÔ∏è IMAGO Core Structure

### High-Level Input

- **TEXT QUERY**  
  Parses user language or natural language commands into intent representations.  
  Connected with an sLLM-based text query and answering system.

- **SEGO MODULE**  
  Provides semantic information via scene graphs.  
  Utilizes SEGO Scene Graph and Ontology Database.

---

### Control Layer

- **Adaptive Compensator (Pole-Zero Autonomous)**  
  Real-time error correction based on pole-zero compensation.  
  Receives rule/state feedback from the LOGOS MODULE for compensator adjustment.

- **Self-Adjusting Controller**  
  Adaptive control optimization.

- **Real-Time Motion Executor**  
  Executes final motion commands at the physical level.

---

### LOGOS Interaction

- **Adaptive Compensator & LOGOS MODULE:**  
  Pole-zero compensation rule exchange  
  Real-time rule optimization  
  Meta-level reasoning feedback  
  Handshake with real-time executor  

---

## ‚öôÔ∏è Robot Arm Requirements

‚úÖ Provides joint-level control interface (Torque / Velocity / Position)  
‚úÖ Real-time control loop period <1ms (RTOS or RT-capable)  
‚úÖ Modifiable controller (Open Source or SDK provided)  
‚úÖ Access to high-performance sensors/drivers (Force-Torque, Encoders)  
‚úÖ Easy ROS2 or RTAPI integration  
‚úÖ Low-level bus access (e.g., EtherCAT, CANopen)  
‚úÖ Public SDK / HAL available  

üëâ Target robot arm: **Franka Emika Panda**

---

## ‚è± Delay Issues and Architectural Limitations

- `libfranka API` provides access above the HAL wrapping layer.  
- Torque commands are injected after internal impedance safety layers.  
- Latency accumulates through ROS2 ‚Üí API ‚Üí internal RT thread ‚Üí driver stack.  
- Estimated accumulated delay: **3‚Äì5 ms**, which threatens phase margin and may induce oscillations.

---

## üìê IMAGO Control Formulation

### Plant Model

$$
G(s) = \frac{1}{J s^2 + B s}
$$

---

### Pole-Zero Compensator Design

Ideal design target:

$$
C(s) G(s) = 1
$$

Actual compensator:

$$
C(s) = J' s^2 + B' s
$$

---

### Delay-Aware Controller

With delay:

$$
G_d(s) = G(s) e^{-s \Delta t}
$$

Effective compensator:

$$
C(s) G_d(s) = C(s) G(s) e^{-s \Delta t}
$$

Phase margin degradation:

$$
\Phi_{\text{margin,eff}} = \Phi_{\text{design}} - 360 f \Delta t
$$

---

### Delay Compensation

Lead compensator form:

$$
C_{\text{delay}}(s) = \frac{\alpha T_{\text{lead}} s + 1}{T_{\text{lead}} s + 1}, \quad 0 < \alpha < 1
$$

Smith predictor (model-based):

$$
\tau_{\text{cmd}}(s) = C(s) \frac{G(s)}{G_d(s)} r(s)
$$

Final torque command:

$$
\tau_{\text{cmd}}(s) = \left( J' s^2 + B' s \right) C_{\text{delay}}(s) e^{+s \Delta t} r(s)
$$

---

### Wrapping Transfer Function

$$
\tau_{\text{actual}}(s) = H_{\text{int}}(s) C(s) C_{\text{delay}}(s) e^{+s \Delta t} r(s)
$$

Where:

$$
H_{\text{int}}(s) = \frac{\theta(s)}{\tau(s)} = \frac{1}{M s^2 + B s}
$$

---

### Nonlinear MIMO Dynamics

$$
M(\theta) \ddot{\theta} + C(\theta, \dot{\theta}) \dot{\theta} + G(\theta) = \tau_{\text{actual}}
$$

$$
M(\theta) = \sum_{i=1}^6 M_i(\theta_1, \dots, \theta_i)
$$

$$
C(\theta, \dot{\theta}) = \sum_{i=1}^6 \sum_{j=1}^6 c_{ij}(\theta) \dot{\theta}_j
$$

$$
c_{ij}(\theta) = \frac{1}{2} \left[
\frac{\partial M_{ij}}{\partial \theta_j} +
\frac{\partial M_{ij}}{\partial \theta_i} +
\frac{\partial M_{jj}}{\partial \theta_i}
\right]
$$

$$
G(\theta) = \sum_{i=1}^6 g_i(\theta)
$$

---

## üîß Self-Adjusting Adaptive Control Law

Adaptive update:

$$
\dot{K} = -\alpha e(t) \dot{e}(t)
$$

Lyapunov function:

$$
V = \frac{1}{2} e^\top P e, \quad \dot{V} = -e^\top Q e \leq 0
$$

With delay compensation:

$$
e_J(t - \Delta t), \quad e_B(t - \Delta t)
$$

---

## ‚öôÔ∏è Multi-Robot End-Effector Plan Synchronization

Global plan reference:

$$
\chi_d^{\text{ref}}(t)
$$

Synchronization constraint:

$$
\forall i,j: 
\left\| \chi_d^{\text{ee,i}}(t) - \chi_d^{\text{ref}}(t) \right\| < \varepsilon_{\text{sync},ij} \quad \wedge \quad 
\left| \Delta t^i - \Delta t^j \right| < \varepsilon_{\text{latency}}
$$

Synchronization torque:

$$
\tau_{\text{actual}}^i(s) \leftarrow \tau_{\text{actual}}^i(s) +
K_{\text{sync}} \sum_j \left( \chi_d^{\text{ee,j}}(t) - \chi_d^{\text{ee,i}}(t) \right)
$$

---

## ‚öôÔ∏è Final IMAGO Wrapping Control Structure

$$
\tau_{\text{actual}}(s) = H_{\text{int}}(s) C^{\text{ML}}(s) C_{\text{delay}}(s) e^{+s \Delta t} \mathcal{L} \theta_d^{\text{final}}(t)
$$

---

## Notes

- **Franka internal controller:**  
  RTOS + safety layers + motor driver layers ‚Üí user control applies above HAL.  
  Internal safety core modification is not allowed.

- **libfranka loop:**  
  1 kHz control loop ‚Üí 1 ms period  
  ROS2 ‚Üí C++ API call ‚Üí internal RT thread  

- **Delay accumulation risk:**  
  Accumulated delay of several ms reduces phase margin, induces oscillation or overshoot.

---

## Potential Extended Formulations

- Multi-joint Smith predictor:

$$
\tau_{\text{cmd}}(s) = \sum_{i=1}^6 C_i(s) \frac{G_i(s)}{G_{d,i}(s)} r_i(s)
$$

- Frequency response with delay:

$$
|C(j\omega) G(j\omega)| \angle e^{-j \omega \Delta t}
$$

- Phase margin with frequency dependency:

$$
\Phi_{\text{margin,eff}}(\omega) = \Phi_{\text{design}}(\omega) - 360 \frac{\omega}{2 \pi} \Delta t
$$

---

``` css
imago_controller/
‚îú‚îÄ‚îÄ CMakeLists.txt                  # ROS2 + C++ ÎπåÎìú ÌååÏùº
‚îú‚îÄ‚îÄ package.xml                      # ROS2 Ìå®ÌÇ§ÏßÄ Î©îÌÉÄ Ï†ïÎ≥¥
‚îú‚îÄ‚îÄ launch/
‚îÇ   ‚îî‚îÄ‚îÄ imago_controller.launch.py   # ROS2 launch ÌååÏùº
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ controller_params.yaml       # IMAGO ÌååÎùºÎØ∏ÌÑ∞ (J, B, delay Îì±)
‚îÇ   ‚îî‚îÄ‚îÄ sync_params.yaml             # Îã§Ï§ë Î°úÎ¥á sync ÌååÎùºÎØ∏ÌÑ∞
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ imago_controller/
‚îÇ       ‚îú‚îÄ‚îÄ imago_controller.hpp          # Î©îÏù∏ ÌÅ¥ÎûòÏä§ ÏÑ†Ïñ∏
‚îÇ       ‚îú‚îÄ‚îÄ pole_zero_compensator.hpp     # Pole-Zero Î≥¥ÏÉÅÍ∏∞ ÌÅ¥ÎûòÏä§
‚îÇ       ‚îú‚îÄ‚îÄ delay_compensator.hpp         # Delay-aware Î≥¥ÏÉÅÍ∏∞ ÌÅ¥ÎûòÏä§
‚îÇ       ‚îú‚îÄ‚îÄ ml_estimator.hpp              # ML Í∏∞Î∞ò inertia/Coriolis/gravity Ï∂îÏ†ï
‚îÇ       ‚îú‚îÄ‚îÄ sync_manager.hpp              # Îã§Ï§ë Î°úÎ¥á sync Í¥ÄÎ¶¨
‚îÇ       ‚îú‚îÄ‚îÄ low_level_interface.hpp       # libfranka / EtherCAT Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
‚îÇ       ‚îî‚îÄ‚îÄ utils.hpp                     # Í≥µÌÜµ Ìï®Ïàò (matrix ops, logging Îì±)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ imago_controller.cpp              # Î©îÏù∏ control loop
‚îÇ   ‚îú‚îÄ‚îÄ pole_zero_compensator.cpp
‚îÇ   ‚îú‚îÄ‚îÄ delay_compensator.cpp
‚îÇ   ‚îú‚îÄ‚îÄ ml_estimator.cpp
‚îÇ   ‚îú‚îÄ‚îÄ sync_manager.cpp
‚îÇ   ‚îú‚îÄ‚îÄ low_level_interface.cpp
‚îÇ   ‚îî‚îÄ‚îÄ main.cpp                          # ROS2 ÎÖ∏Îìú ÏßÑÏûÖÏ†ê
‚îú‚îÄ‚îÄ urdf/
‚îÇ   ‚îî‚îÄ‚îÄ panda_with_imago.urdf.xacro       # Panda + IMAGO Ï∂îÍ∞Ä ÎßÅÌÅ¨/ÏÑºÏÑú URDF
‚îú‚îÄ‚îÄ meshes/
‚îÇ   ‚îî‚îÄ‚îÄ (ÌïÑÏöî Ïãú custom link/EE mesh ÌååÏùº)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ test_plot.py                      # Latency/torque/sync Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_imago_controller.cpp         # gtest Í∏∞Î∞ò Ïú†ÎãõÌÖåÏä§Ìä∏
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ design_overview.md                # ÏÑ§Í≥Ñ Î¨∏ÏÑú
‚îú‚îÄ‚îÄ data_logs/
‚îÇ   ‚îî‚îÄ‚îÄ (Ïã§Ìóò Ï§ë ÏÉùÏÑ±ÎêòÎäî log ÌååÏùº Ï†ÄÏû•)
‚îî‚îÄ‚îÄ README.md                             # Ï†ÑÏ≤¥ Í∞úÏöî ÏÑ§Î™Ö


```
